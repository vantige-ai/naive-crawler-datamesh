#!/bin/bash

# =============================================================================
# GCP Serverless Web Crawler - Initial Setup Script
# =============================================================================
# This script helps new users get started with the crawler by:
# 1. Configuring their project-specific values
# 2. Generating all documentation with their settings
# 3. Creating their config.sh file
# =============================================================================

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Print functions
print_header() {
    echo -e "${CYAN}============================================${NC}"
    echo -e "${CYAN}$1${NC}"
    echo -e "${CYAN}============================================${NC}"
}

print_status() {
    echo -e "${GREEN}✅${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}⚠️${NC} $1"
}

print_error() {
    echo -e "${RED}❌${NC} $1"
}

print_info() {
    echo -e "${BLUE}ℹ️${NC} $1"
}

print_question() {
    echo -e "${YELLOW}❓${NC} $1"
}

# Get project root
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

print_header "GCP Serverless Web Crawler Setup"
echo ""
print_info "This script will help you configure the crawler for your project."
echo ""

# Check if already configured
if [[ -f "$PROJECT_ROOT/config.sh" && -f "$PROJECT_ROOT/variables.sh" ]]; then
    print_warning "Configuration files already exist."
    echo ""
    read -p "Do you want to reconfigure? (y/N): " reconfigure
    if [[ ! "$reconfigure" =~ ^[Yy]$ ]]; then
        print_info "Setup cancelled. To reconfigure later, run: ./setup.sh"
        exit 0
    fi
fi

echo ""
print_header "Project Configuration"
echo ""

# Collect user input
print_question "Enter your GCP Project ID:"
read -p "Project ID: " PROJECT_ID

print_question "Enter the domain you want to crawl (without https://):"
read -p "Domain: " DOMAIN

print_question "Enter your Firecrawl API key (get one from https://firecrawl.dev):"
read -p "API Key: " API_KEY

print_question "Enter your preferred GCP region (default: us-central1):"
read -p "Region [us-central1]: " REGION
REGION=${REGION:-us-central1}

print_question "Enter a unique crawler ID (default: crawler-basic):"
read -p "Crawler ID [crawler-basic]: " CRAWLER_ID
CRAWLER_ID=${CRAWLER_ID:-crawler-basic}

print_question "Enter max pages to discover (default: 10):"
read -p "Page Limit [10]: " PAGE_LIMIT
PAGE_LIMIT=${PAGE_LIMIT:-10}

print_question "Enter your company/organization name (for LICENSE):"
read -p "Company Name: " COMPANY_NAME

echo ""
print_header "Configuration Summary"
echo ""
echo "Project ID:    $PROJECT_ID"
echo "Domain:        $DOMAIN"
echo "API Key:       ${API_KEY:0:10}... (hidden)"
echo "Region:        $REGION"
echo "Crawler ID:    $CRAWLER_ID"
echo "Page Limit:    $PAGE_LIMIT"
echo "Company:       $COMPANY_NAME"
echo ""

read -p "Is this configuration correct? (y/N): " confirm
if [[ ! "$confirm" =~ ^[Yy]$ ]]; then
    print_error "Setup cancelled. Run ./setup.sh again to reconfigure."
    exit 1
fi

echo ""
print_header "Generating Configuration Files"
echo ""

# Update variables.sh
print_info "Updating variables.sh..."
cat > "$PROJECT_ROOT/variables.sh" << EOF
# =============================================================================
# GCP Serverless Web Crawler - Centralized Configuration
# =============================================================================
# This file contains all user-configurable variables that will be used to
# generate documentation, examples, and configuration templates.
# 
# Generated by setup.sh on $(date)
# =============================================================================

# --- PRIMARY CONFIGURATION ---
export EXAMPLE_PROJECT_ID="$PROJECT_ID"
export EXAMPLE_DOMAIN="$DOMAIN"
export EXAMPLE_API_KEY="$API_KEY"
export EXAMPLE_REGION="$REGION"

# Company/Organization info
export COMPANY_NAME="$COMPANY_NAME"
export REPOSITORY_NAME="gcp-serverless-web-crawler"

# --- DEPLOYMENT DEFAULTS ---
export DEFAULT_CRAWLER_ID="$CRAWLER_ID"
export DEFAULT_PAGE_LIMIT="$PAGE_LIMIT"
export DEFAULT_REGION="$REGION"

# --- DOCUMENTATION SETTINGS ---
export INCLUDE_PRODUCTION_EXAMPLES="true"
export INCLUDE_TROUBLESHOOTING="true"
export INCLUDE_ADVANCED_CONFIG="true"

# --- GENERATED VALUES ---
export EXAMPLE_BIGQUERY_TABLE="\${EXAMPLE_PROJECT_ID}.crawler.\${DEFAULT_CRAWLER_ID//-/_}_raw"
export EXAMPLE_SERVICE_URL="https://\${DEFAULT_CRAWLER_ID}-mapper-svc-uhqeuxlwfq-uc.a.run.app"
export EXAMPLE_TOPIC_NAME="\${DEFAULT_CRAWLER_ID}-input-topic"

# =============================================================================
# Documentation Generator Configuration
# =============================================================================
export TEMPLATE_DIR="templates"
export OUTPUT_DIR="."
export BACKUP_DIR=".backup"

export TEMPLATE_FILES=(
    "README.md"
    "architecture.md" 
    "config.sh"
    ".env.example"
    "scripts/bigquery_views.sql"
)

# Validation function
validate_config() {
    return 0  # User has already provided custom values
}
EOF

print_status "Updated variables.sh"

# Generate documentation using the generator script
print_info "Generating documentation files..."
if [[ -f "$PROJECT_ROOT/scripts/generate_docs.sh" ]]; then
    cd "$PROJECT_ROOT"
    ./scripts/generate_docs.sh --force
    print_status "Documentation generated"
else
    print_warning "Documentation generator not found, creating config.sh manually..."
    
    # Create config.sh manually if generator doesn't exist
    cat > "$PROJECT_ROOT/config.sh" << EOF
# --- CORE CONFIGURATION (EDIT THESE VALUES) ---

# A unique ID for this crawler instance. Use lowercase letters, numbers, and hyphens.
export CRAWLER_ID="$CRAWLER_ID"

# The root domain to crawl.
export DOMAIN_TO_CRAWL="$DOMAIN"

# Max pages to map using Firecrawl.
export PAGE_LIMIT="$PAGE_LIMIT"

# The GCP Project ID.
export PROJECT_ID="$PROJECT_ID"
export REGION="$REGION"

# IMPORTANT: You must set this value before deploying.
export FIRECRAWL_API_KEY="$API_KEY"

# --- DERIVED RESOURCE NAMES (DO NOT EDIT) ---
# These names are constructed from the CRAWLER_ID for consistency.

# Service Accounts
export MAPPER_SA_NAME="\${CRAWLER_ID}-mapper-sa"
export PROCESSOR_SA_NAME="\${CRAWLER_ID}-processor-sa"
export MAPPER_SA_EMAIL="\${MAPPER_SA_NAME}@\${PROJECT_ID}.iam.gserviceaccount.com"
export PROCESSOR_SA_EMAIL="\${PROCESSOR_SA_NAME}@\${PROJECT_ID}.iam.gserviceaccount.com"

# Cloud Run Services
export URL_MAPPER_SERVICE_NAME="\${CRAWLER_ID}-mapper-svc"
export PAGE_PROCESSOR_SERVICE_NAME="\${CRAWLER_ID}-processor-svc"

# Pub/Sub Topics
export INPUT_TOPIC_ID="\${CRAWLER_ID}-input-topic"
export URL_TOPIC_ID="\${CRAWLER_ID}-urls-topic"
export OUTPUT_TOPIC_ID="\${CRAWLER_ID}-output-topic"

# Pub/Sub Subscriptions
export MAPPER_SUBSCRIPTION_ID="\${CRAWLER_ID}-mapper-sub"
export PROCESSOR_SUBSCRIPTION_ID="\${CRAWLER_ID}-processor-sub"

# BigQuery Configuration
export BQ_DATASET="crawler"  # Standardized dataset name
export BQ_RAW_TABLE="\${CRAWLER_ID//-/_}_raw"    # Raw Pub/Sub messages with metadata
export BQ_PROCESSED_TABLE="\${CRAWLER_ID//-/_}_processed"  # Structured processed data

# BigQuery Subscriptions (for direct Pub/Sub -> BigQuery integration)
export RAW_DATA_SUBSCRIPTION_ID="\${CRAWLER_ID}-raw-data-sub"

# Security and IAM Configuration
export ALLOW_UNAUTHENTICATED="true"   # Required: Pub/Sub push subscriptions need unauthenticated access with OIDC tokens
EOF
    print_status "Created config.sh"
fi

# Update .gitignore to ensure config.sh is ignored
if ! grep -q "^config\.sh$" "$PROJECT_ROOT/.gitignore" 2>/dev/null; then
    echo "config.sh" >> "$PROJECT_ROOT/.gitignore"
    print_status "Added config.sh to .gitignore"
fi

echo ""
print_header "Setup Complete!"
echo ""
print_status "Configuration files created:"
echo "  ✅ variables.sh (centralized configuration)"
echo "  ✅ config.sh (deployment configuration)"
echo "  ✅ Updated documentation files"
echo ""
print_info "Next steps:"
echo "  1. Review your configuration: cat config.sh"
echo "  2. Deploy infrastructure: ./scripts/setup_infra.sh"
echo "  3. Deploy services: ./scripts/deploy.sh"
echo "  4. Test the crawler:"
echo "     gcloud pubsub topics publish $CRAWLER_ID-input-topic \\"
echo "       --message '{\"domain\": \"$DOMAIN\"}' \\"
echo "       --project=$PROJECT_ID"
echo ""
print_warning "Security reminder:"
echo "  - config.sh contains your API key and is ignored by git"
echo "  - Never commit real API keys to version control"
echo "  - Share only the template files for collaboration"
echo ""
print_info "To reconfigure later, run: ./setup.sh"