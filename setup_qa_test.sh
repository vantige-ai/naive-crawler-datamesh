#!/bin/bash

# =============================================================================
# QA Deployment Test Script for GCP Serverless Web Crawler
# =============================================================================
# This script creates a real QA deployment using:
# - Same Firebase API key and Google Cloud project as production
# - qa-crawler naming for all resources  
# - crawler.qa_table as BigQuery destination
# - All files saved to qa_test/ folder for isolation
# =============================================================================

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Print functions
print_header() {
    echo -e "${CYAN}============================================${NC}"
    echo -e "${CYAN}$1${NC}"
    echo -e "${CYAN}============================================${NC}"
}

print_status() {
    echo -e "${GREEN}âœ…${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸${NC} $1"
}

print_error() {
    echo -e "${RED}âŒ${NC} $1"
}

print_info() {
    echo -e "${BLUE}â„¹ï¸${NC} $1"
}

# Get project root and check for original config
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
QA_TEST_DIR="$PROJECT_ROOT/qa_test"

print_header "QA Deployment Test - Real Environment"
echo ""

# Check if original config.sh exists to get real values
if [[ ! -f "$PROJECT_ROOT/config.sh" ]]; then
    print_error "config.sh not found! Please run ./setup.sh first to create your configuration."
    print_info "This test script needs your real project values to create a QA deployment."
    exit 1
fi

# Load the real configuration values
print_info "Loading real configuration from config.sh..."
source "$PROJECT_ROOT/config.sh"

# Capture the real values
REAL_PROJECT_ID="$PROJECT_ID"
REAL_FIRECRAWL_API_KEY="$FIRECRAWL_API_KEY"
REAL_DOMAIN="$DOMAIN_TO_CRAWL"
REAL_REGION="$REGION"

print_status "Loaded real configuration:"
echo "  Project ID: $REAL_PROJECT_ID"
echo "  Domain: $REAL_DOMAIN"
echo "  API Key: ${REAL_FIRECRAWL_API_KEY:0:10}... (hidden)"
echo "  Region: $REAL_REGION"
echo ""

# Clean and create QA test directory
if [[ -d "$QA_TEST_DIR" ]]; then
    print_warning "QA test directory exists. Cleaning..."
    rm -rf "$QA_TEST_DIR"
fi

mkdir -p "$QA_TEST_DIR"
print_status "Created QA test directory: $QA_TEST_DIR"

echo ""
print_header "QA Test Configuration"
echo ""
print_info "Creating QA deployment with:"
echo "  Crawler ID:    qa-crawler"
echo "  Project ID:    $REAL_PROJECT_ID (REAL)"
echo "  Domain:        $REAL_DOMAIN (REAL)"
echo "  API Key:       ${REAL_FIRECRAWL_API_KEY:0:10}... (REAL)"
echo "  Region:        $REAL_REGION"
echo "  BigQuery:      crawler.qa_table"
echo ""

# Generate QA config.sh with real values but qa-crawler naming
print_info "Generating QA config.sh..."
cat > "$QA_TEST_DIR/config.sh" << EOF
# --- QA TEST CONFIGURATION ---
# Generated by setup_qa_test.sh on $(date)
# Uses REAL credentials but qa-crawler naming and qa_table destination

# A unique ID for this crawler instance. Use lowercase letters, numbers, and hyphens.
export CRAWLER_ID="qa-crawler"

# The root domain to crawl.
export DOMAIN_TO_CRAWL="$REAL_DOMAIN"

# Max pages to map using Firecrawl.
export PAGE_LIMIT="10"

# The GCP Project ID.
export PROJECT_ID="$REAL_PROJECT_ID"
export REGION="$REAL_REGION"

# IMPORTANT: You must set this value before deploying.
export FIRECRAWL_API_KEY="$REAL_FIRECRAWL_API_KEY"

# --- DERIVED RESOURCE NAMES (DO NOT EDIT) ---
# These names are constructed from the CRAWLER_ID for consistency.

# Service Accounts
export MAPPER_SA_NAME="\${CRAWLER_ID}-mapper-sa"
export PROCESSOR_SA_NAME="\${CRAWLER_ID}-processor-sa"
export MAPPER_SA_EMAIL="\${MAPPER_SA_NAME}@\${PROJECT_ID}.iam.gserviceaccount.com"
export PROCESSOR_SA_EMAIL="\${PROCESSOR_SA_NAME}@\${PROJECT_ID}.iam.gserviceaccount.com"

# Cloud Run Services
export URL_MAPPER_SERVICE_NAME="\${CRAWLER_ID}-mapper-svc"
export PAGE_PROCESSOR_SERVICE_NAME="\${CRAWLER_ID}-processor-svc"

# Pub/Sub Topics
export INPUT_TOPIC_ID="\${CRAWLER_ID}-input-topic"
export URL_TOPIC_ID="\${CRAWLER_ID}-urls-topic"
export OUTPUT_TOPIC_ID="\${CRAWLER_ID}-output-topic"

# Pub/Sub Subscriptions
export MAPPER_SUBSCRIPTION_ID="\${CRAWLER_ID}-mapper-sub"
export PROCESSOR_SUBSCRIPTION_ID="\${CRAWLER_ID}-processor-sub"

# BigQuery Configuration - QA SPECIFIC TABLES
export BQ_DATASET="crawler"  # Same dataset as production
export BQ_RAW_TABLE="qa_table"    # QA-specific table name
export BQ_PROCESSED_TABLE="qa_processed"  # QA-specific processed table

# BigQuery Subscriptions (for direct Pub/Sub -> BigQuery integration)
export RAW_DATA_SUBSCRIPTION_ID="\${CRAWLER_ID}-raw-data-sub"

# Security and IAM Configuration
export ALLOW_UNAUTHENTICATED="true"   # Required: Pub/Sub push subscriptions need unauthenticated access with OIDC tokens
EOF

print_status "Generated QA config.sh with real credentials and qa-crawler naming"

# Create deployment scripts in QA directory
print_info "Copying deployment scripts to QA directory..."
mkdir -p "$QA_TEST_DIR/scripts"

# Copy and modify setup_infra.sh for QA
cp "$PROJECT_ROOT/scripts/setup_infra.sh" "$QA_TEST_DIR/scripts/setup_infra.sh"
cp "$PROJECT_ROOT/scripts/deploy.sh" "$QA_TEST_DIR/scripts/deploy.sh"
cp "$PROJECT_ROOT/scripts/teardown.sh" "$QA_TEST_DIR/scripts/teardown.sh"

# Create QA-specific BigQuery views
cat > "$QA_TEST_DIR/scripts/bigquery_views.sql" << EOF
-- BigQuery Views for QA Crawler
-- This script creates views to unpack and analyze data from the qa_table

-- 1. Main parsed messages view - extracts all fields from JSON data
CREATE OR REPLACE MATERIALIZED VIEW \`$REAL_PROJECT_ID.crawler.qa_parsed_messages\` AS
SELECT 
  subscription_name,
  message_id,
  publish_time,
  -- Extract fields from the JSON data column
  JSON_VALUE(data, '\$.url') as url,
  JSON_VALUE(data, '\$.crawler_id') as crawler_id,
  JSON_VALUE(data, '\$.domain') as domain,
  JSON_VALUE(data, '\$.status') as status,
  -- Handle nanosecond precision timestamps by truncating to microseconds
  PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E6SZ', SUBSTR(JSON_VALUE(data, '\$.timestamp'), 1, 27) || 'Z') as processed_timestamp,
  JSON_VALUE(data, '\$.markdown') as markdown_content,
  LENGTH(JSON_VALUE(data, '\$.markdown')) as markdown_length,
  -- Calculate processing time
  TIMESTAMP_DIFF(
    publish_time, 
    PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E6SZ', SUBSTR(JSON_VALUE(data, '\$.timestamp'), 1, 27) || 'Z'), 
    SECOND
  ) as processing_seconds,
  -- Extract attributes if any
  attributes
FROM 
  \`$REAL_PROJECT_ID.crawler.qa_table\`
WHERE 
  JSON_VALUE(data, '\$.url') IS NOT NULL
;

-- 2. QA Daily Stats
CREATE OR REPLACE MATERIALIZED VIEW \`$REAL_PROJECT_ID.crawler.qa_daily_stats\` AS
SELECT 
  JSON_VALUE(data, '\$.crawler_id') as crawler_id,
  JSON_VALUE(data, '\$.domain') as domain,
  DATE(publish_time) as crawl_date,
  COUNT(*) as total_pages,
  COUNTIF(JSON_VALUE(data, '\$.status') = 'success') as successful_pages,
  COUNTIF(JSON_VALUE(data, '\$.status') = 'error') as failed_pages,
  AVG(LENGTH(JSON_VALUE(data, '\$.markdown'))) as avg_content_length,
  MAX(LENGTH(JSON_VALUE(data, '\$.markdown'))) as max_content_length,
  MIN(publish_time) as first_crawl_time,
  MAX(publish_time) as last_crawl_time
FROM 
  \`$REAL_PROJECT_ID.crawler.qa_table\`
WHERE 
  JSON_VALUE(data, '\$.url') IS NOT NULL
GROUP BY 
  1, 2, 3
;
EOF

print_status "Generated QA-specific BigQuery views"

# Create QA test script
cat > "$QA_TEST_DIR/test_qa_deployment.sh" << 'EOF'
#!/bin/bash

# QA Deployment Test Script
set -e

# Colors
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m'

print_status() { echo -e "${GREEN}âœ…${NC} $1"; }
print_info() { echo -e "${BLUE}â„¹ï¸${NC} $1"; }
print_warning() { echo -e "${YELLOW}âš ï¸${NC} $1"; }

# Load QA configuration
source config.sh

echo "ðŸš€ QA Deployment Test"
echo "Crawler ID: $CRAWLER_ID"
echo "Project: $PROJECT_ID"
echo "Domain: $DOMAIN_TO_CRAWL"
echo ""

print_info "1. Setting up QA infrastructure..."
./scripts/setup_infra.sh

print_info "2. Deploying QA services..."
./scripts/deploy.sh

print_info "3. Testing QA crawler with sample message..."
gcloud pubsub topics publish $INPUT_TOPIC_ID \
  --message '{"domain": "'$DOMAIN_TO_CRAWL'", "uid": "qa-test-001"}' \
  --project=$PROJECT_ID

print_status "QA deployment test complete!"
print_info "Monitor BigQuery table: $PROJECT_ID.crawler.qa_table"
print_info "Check logs with:"
echo "  gcloud logging read \"resource.type=cloud_run_revision AND resource.labels.service_name=$URL_MAPPER_SERVICE_NAME\" --limit=10 --project=$PROJECT_ID"

print_warning "To teardown QA resources: ./scripts/teardown.sh"
EOF

chmod +x "$QA_TEST_DIR/test_qa_deployment.sh"
print_status "Created QA deployment test script"

# Create monitoring script
cat > "$QA_TEST_DIR/monitor_qa.sh" << EOF
#!/bin/bash

# QA Monitoring Script
source config.sh

echo "ðŸ” QA Crawler Monitoring"
echo "========================"

echo ""
echo "ðŸ“Š BigQuery QA Table Status:"
bq query --use_legacy_sql=false --project_id=\$PROJECT_ID \\
  'SELECT COUNT(*) as total_messages, MAX(publish_time) as latest_message 
   FROM \`$REAL_PROJECT_ID.crawler.qa_table\` 
   WHERE DATE(publish_time) = CURRENT_DATE()'

echo ""
echo "ðŸ“ˆ QA Status Breakdown:"
bq query --use_legacy_sql=false --project_id=\$PROJECT_ID \\
  'SELECT JSON_EXTRACT_SCALAR(data, "\$.status") as status, COUNT(*) as count
   FROM \`$REAL_PROJECT_ID.crawler.qa_table\` 
   WHERE DATE(publish_time) = CURRENT_DATE()
   GROUP BY status ORDER BY count DESC'

echo ""
echo "ðŸƒ Recent QA Activity:"
bq query --use_legacy_sql=false --project_id=\$PROJECT_ID \\
  'SELECT JSON_EXTRACT_SCALAR(data, "\$.url") as url,
          JSON_EXTRACT_SCALAR(data, "\$.status") as status,
          publish_time
   FROM \`$REAL_PROJECT_ID.crawler.qa_table\` 
   WHERE DATE(publish_time) = CURRENT_DATE()
   ORDER BY publish_time DESC LIMIT 5'
EOF

chmod +x "$QA_TEST_DIR/monitor_qa.sh"
print_status "Created QA monitoring script"

echo ""
print_header "QA Test Environment Ready"
echo ""
print_status "ðŸŽ‰ QA deployment environment created successfully!"
echo ""
print_info "Generated files in $QA_TEST_DIR:"
echo "  ðŸ“„ config.sh (QA configuration with real credentials)"
echo "  ðŸ“„ scripts/ (deployment scripts)"
echo "  ðŸ“„ test_qa_deployment.sh (run full QA deployment)"
echo "  ðŸ“„ monitor_qa.sh (monitor QA crawler activity)"
echo ""
print_info "To deploy and test QA crawler:"
echo "  cd qa_test"
echo "  ./test_qa_deployment.sh"
echo ""
print_info "To monitor QA activity:"
echo "  cd qa_test"
echo "  ./monitor_qa.sh"
echo ""
print_warning "QA Environment Details:"
echo "  â€¢ Uses REAL project: $REAL_PROJECT_ID"
echo "  â€¢ Uses REAL API key: ${REAL_FIRECRAWL_API_KEY:0:10}..."
echo "  â€¢ Crawler ID: qa-crawler"
echo "  â€¢ BigQuery destination: crawler.qa_table"
echo "  â€¢ All resources prefixed with 'qa-crawler'"
echo ""
print_warning "Remember: This creates real GCP resources that may incur charges!"